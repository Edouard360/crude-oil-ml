{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crude Oil Challenge\n",
    "## About\n",
    "This notebook presents my approach to the [crude oil challenge][1] proposed by Societe General.<br>\n",
    "My current score on the leaderboard is 0.80, the regressor used being the RandomForestClassifier of sklearn.\n",
    "<br><br> _NB:_ <br>\n",
    "I created a Github repository for this project, that provides a better insight into the structure of my work. [Check it out][2] ! \n",
    "<br>\n",
    "\n",
    "[1]: https://challengedata.ens.fr/en/challenge/25/predict_the_crude_oil_production_trend.html#\n",
    "[2]: https://github.com/Edouard360/crude-oil-ml\n",
    "\n",
    "## First Look Into The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"../data/train.csv\",delimiter=\";\",header=0,index_col=0);\n",
    "train_label = pd.read_csv(\"../data/label.csv\",delimiter=\";\",header=0,index_col=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the shape of the data ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10159, 122)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling the column names\n",
    "We can identify columns groups from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_group = [train_df.columns[2+10*i:2+10*(i+1)] for i in range(12)]\n",
    "columns_group.insert(0,train_df.columns[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that way, we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['1_diffClosing stocks(kmt)', '1_diffExports(kmt)', '1_diffImports(kmt)',\n",
       "       '1_diffRefinery intake(kmt)', '1_diffWTI',\n",
       "       '1_diffSumClosing stocks(kmt)', '1_diffSumExports(kmt)',\n",
       "       '1_diffSumImports(kmt)', '1_diffSumProduction(kmt)',\n",
       "       '1_diffSumRefinery intake(kmt)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_group[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['2_diffClosing stocks(kmt)', '2_diffExports(kmt)', '2_diffImports(kmt)',\n",
       "       '2_diffRefinery intake(kmt)', '2_diffWTI',\n",
       "       '2_diffSumClosing stocks(kmt)', '2_diffSumExports(kmt)',\n",
       "       '2_diffSumImports(kmt)', '2_diffSumProduction(kmt)',\n",
       "       '2_diffSumRefinery intake(kmt)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_group[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create useful functions to __operate on these columns__. For instance, we'd like to be able to get:\n",
    "\n",
    "+ All the statistics for one period. Just like the columns_group[1] output.\n",
    "+ All the statistics for many periods. The period set could be expressed like: `[1,2,3,4]` or `range(1,5)`.\n",
    "+ The statistics for only one category ( like `'diffImports(kmt)'` ) over all the periods (period1 to period12).\n",
    "+ The statistics for a list of categories ( like `['diffImports(kmt)','diffWTI','diffExports']` ) over all the periods.\n",
    "+ The statistics for a list of categories ( like `['diffImports(kmt)','diffWTI','diffExports']` ) for some given periods ( like `range(4,8)` ).\n",
    "\n",
    "The following lines of code provide these features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COLUMNS_SUFFIX = ['_diffClosing stocks(kmt)',\n",
    "                  '_diffExports(kmt)', '_diffImports(kmt)',\n",
    "                  '_diffRefinery intake(kmt)', '_diffWTI',\n",
    "                  '_diffSumClosing stocks(kmt)', '_diffSumExports(kmt)',\n",
    "                  '_diffSumImports(kmt)', '_diffSumProduction(kmt)',\n",
    "                  '_diffSumRefinery intake(kmt)']\n",
    "\n",
    "\n",
    "def check_type_prefix(i):\n",
    "    if (type(i) is int):\n",
    "        i = [i]\n",
    "    elif ((type(i) is not range) and (type(i) is not list)):\n",
    "        raise TypeError(\"Only range, list or single integer allowed !\")\n",
    "    return i\n",
    "\n",
    "\n",
    "def check_type_suffix(i):\n",
    "    if (type(i) is str):\n",
    "        i = [i]\n",
    "    elif ((type(i) is not list)):\n",
    "        raise TypeError(\"Only list or single string allowed !\")\n",
    "    return i\n",
    "\n",
    "\n",
    "def get_prefix(i):\n",
    "    i = check_type_prefix(i)\n",
    "    return [str(j) + suffix for j in i for suffix in COLUMNS_SUFFIX]\n",
    "\n",
    "\n",
    "def get_suffix(suffix_request, prefix=range(1, 13)):\n",
    "    prefix = check_type_prefix(prefix)\n",
    "    suffix_request = check_type_suffix(suffix_request)\n",
    "    for i in range(len(suffix_request)):\n",
    "        for original_suffix in COLUMNS_SUFFIX:\n",
    "            if (str.lower(original_suffix).find(str.lower(suffix_request[i])) != -1):\n",
    "                suffix_request[i] = original_suffix\n",
    "                break\n",
    "    suffix_request = list(set(suffix_request))\n",
    "    return [str(j) + suffix for j in prefix for suffix in suffix_request]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can simply use `get_prefix` and `get_suffix` that way:\n",
    "\n",
    "+ All the statistics for one period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5_diffClosing stocks(kmt)',\n",
       " '5_diffExports(kmt)',\n",
       " '5_diffImports(kmt)',\n",
       " '5_diffRefinery intake(kmt)',\n",
       " '5_diffWTI',\n",
       " '5_diffSumClosing stocks(kmt)',\n",
       " '5_diffSumExports(kmt)',\n",
       " '5_diffSumImports(kmt)',\n",
       " '5_diffSumProduction(kmt)',\n",
       " '5_diffSumRefinery intake(kmt)']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prefix(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ All the statistics for many periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['7_diffClosing stocks(kmt)',\n",
       " '7_diffExports(kmt)',\n",
       " '7_diffImports(kmt)',\n",
       " '7_diffRefinery intake(kmt)',\n",
       " '7_diffWTI',\n",
       " '7_diffSumClosing stocks(kmt)',\n",
       " '7_diffSumExports(kmt)',\n",
       " '7_diffSumImports(kmt)',\n",
       " '7_diffSumProduction(kmt)',\n",
       " '7_diffSumRefinery intake(kmt)',\n",
       " '8_diffClosing stocks(kmt)',\n",
       " '8_diffExports(kmt)',\n",
       " '8_diffImports(kmt)',\n",
       " '8_diffRefinery intake(kmt)',\n",
       " '8_diffWTI',\n",
       " '8_diffSumClosing stocks(kmt)',\n",
       " '8_diffSumExports(kmt)',\n",
       " '8_diffSumImports(kmt)',\n",
       " '8_diffSumProduction(kmt)',\n",
       " '8_diffSumRefinery intake(kmt)']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prefix(range(7,9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The statistics for only one category over all the periods. (Observe the leniency for the name of the feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_diffImports(kmt)',\n",
       " '2_diffImports(kmt)',\n",
       " '3_diffImports(kmt)',\n",
       " '4_diffImports(kmt)',\n",
       " '5_diffImports(kmt)',\n",
       " '6_diffImports(kmt)',\n",
       " '7_diffImports(kmt)',\n",
       " '8_diffImports(kmt)',\n",
       " '9_diffImports(kmt)',\n",
       " '10_diffImports(kmt)',\n",
       " '11_diffImports(kmt)',\n",
       " '12_diffImports(kmt)']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_suffix('Imports') # get_suffix('imports') and get_suffix('_diffImports(kmt)') give the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ The statistics for a list of categories over all the periods. (Again, the function is lenient for the feature's names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_diffExports(kmt)',\n",
       " '1_diffImports(kmt)',\n",
       " '2_diffExports(kmt)',\n",
       " '2_diffImports(kmt)',\n",
       " '3_diffExports(kmt)',\n",
       " '3_diffImports(kmt)',\n",
       " '4_diffExports(kmt)',\n",
       " '4_diffImports(kmt)',\n",
       " '5_diffExports(kmt)',\n",
       " '5_diffImports(kmt)',\n",
       " '6_diffExports(kmt)',\n",
       " '6_diffImports(kmt)',\n",
       " '7_diffExports(kmt)',\n",
       " '7_diffImports(kmt)',\n",
       " '8_diffExports(kmt)',\n",
       " '8_diffImports(kmt)',\n",
       " '9_diffExports(kmt)',\n",
       " '9_diffImports(kmt)',\n",
       " '10_diffExports(kmt)',\n",
       " '10_diffImports(kmt)',\n",
       " '11_diffExports(kmt)',\n",
       " '11_diffImports(kmt)',\n",
       " '12_diffExports(kmt)',\n",
       " '12_diffImports(kmt)']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_suffix(['Imports','Exports'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ And finally, the statistics for a list of categories __for some given periods__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_diffWTI',\n",
       " '1_diffRefinery intake(kmt)',\n",
       " '2_diffWTI',\n",
       " '2_diffRefinery intake(kmt)',\n",
       " '3_diffWTI',\n",
       " '3_diffRefinery intake(kmt)']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_suffix(['wti','refinery'],range(1,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have tools for handling the columns let's move on ! \n",
    "<br><br>_NB:_<br> There is also a `except_suffix` function (that does what its name suggest).\n",
    "[Github link][3] for the full code.\n",
    "\n",
    "[3]: https://github.com/Edouard360/crude-oil-ml/blob/master/tools/features_name.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with missing values\n",
    "We can explicit the columns where the missing values are located :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns where we find missing values :\n",
      "\n",
      "Index(['1_diffClosing stocks(kmt)', '1_diffImports(kmt)',\n",
      "       '2_diffClosing stocks(kmt)', '2_diffImports(kmt)',\n",
      "       '3_diffClosing stocks(kmt)', '3_diffImports(kmt)',\n",
      "       '4_diffClosing stocks(kmt)', '4_diffImports(kmt)',\n",
      "       '5_diffClosing stocks(kmt)', '5_diffImports(kmt)',\n",
      "       '6_diffClosing stocks(kmt)', '6_diffImports(kmt)',\n",
      "       '7_diffClosing stocks(kmt)', '7_diffImports(kmt)',\n",
      "       '8_diffClosing stocks(kmt)', '8_diffImports(kmt)',\n",
      "       '9_diffClosing stocks(kmt)', '9_diffImports(kmt)',\n",
      "       '10_diffClosing stocks(kmt)', '10_diffImports(kmt)',\n",
      "       '11_diffClosing stocks(kmt)', '11_diffImports(kmt)',\n",
      "       '12_diffClosing stocks(kmt)', '12_diffImports(kmt)'],\n",
      "      dtype='object')\n",
      "\n",
      "Total number of missing values :\n",
      "\n",
      "4497\n"
     ]
    }
   ],
   "source": [
    "def discrimateColumns(train_df):\n",
    "    describe_df = train_df.describe()\n",
    "    columns_missing = train_df.columns[(describe_df.loc['count'] < len(train_df)).values]\n",
    "    columns_full = train_df.columns[(describe_df.loc['count'] == len(train_df)).values]\n",
    "    return (columns_missing,columns_full)\n",
    "print(\"Columns where we find missing values :\\n\")\n",
    "print(discrimateColumns(train_df)[0])\n",
    "print(\"\\nTotal number of missing values :\\n\")\n",
    "print(train_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could impute values simply by __choosing the median for each column__.<br>\n",
    "Or we could __group with respect to the country and the month__ and then __impute the median__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# group = [\"country\",\"month\"]\n",
    "# train_df.ix[:,2:] = train_df.groupby(group,as_index=False).transform(lambda x: x.fillna(x.median()))[train_df.columns[2:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yet, is is also very common to __fill the NAs with a preliminary regression__. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "\n",
    "def fillRegression(train_df):\n",
    "    columns_missing,columns_full = discrimateColumns(train_df)\n",
    "    for target_column in columns_missing:\n",
    "        #print(target_column) # Uncomment this to see the progression\n",
    "        X = train_df[columns_full].values\n",
    "        y = train_df[target_column].values\n",
    "        train_is = np.arange(len(y))[~np.isnan(y)]\n",
    "        test_is = np.arange(len(y))[np.isnan(y)]\n",
    "        clf = RandomForestRegressor(n_estimators=10, max_leaf_nodes=5)\n",
    "        clf.fit(X[train_is], y[train_is])\n",
    "        train_df.ix[test_is, target_column] = clf.predict(X[test_is])\n",
    "        columns_full = np.append(columns_full, target_column)\n",
    "    return train_df\n",
    "\n",
    "train_df = fillRegression(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now check that there are no missing values left:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of missing values :\n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTotal number of missing values :\\n\")\n",
    "print(train_df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Periodicity\n",
    " There is actually a __periodicity__ in the data. Of course, there are month, but we can specifically __recover their order__. <br>In this section we engineer a __`'period'` feature__, which ranges from 0 to about 140 for the train_df and corresponds to the:<br> `140 =~ (12 months) * (~11-12 years)`Of the train data. The period of the test data is __also recoverable__ and ranges until:<br>  `170 =~ (12 months) * (~14 years)` Which corresponds to the Societe Generale digits - _from 2002 to 2016_.<br> \n",
    "The code below can take any feature of reference as a input. It builds the dataframe which puts __the value taken by that feature__, side by side with __the period at which the feature took that value__. <br>\n",
    "For the sake of simplicity, I have removed the comments from the function. [Github link][4] for the full code.\n",
    "\n",
    "[4]: https://github.com/Edouard360/crude-oil-ml/blob/master/feature_extraction/time_period.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFirstValue(X_df,suffix_feature):\n",
    "    valuesSumImport = X_df[get_suffix(suffix_feature, 1)[0]].unique()\n",
    "    for value in valuesSumImport:\n",
    "        if (sum(X_df[get_suffix(suffix_feature, 2)[0]] == value) == 0):\n",
    "            return value\n",
    "\n",
    "def computePeriod(X_df, orderedValues,suffix_feature = \"SumImports\"):\n",
    "    if(len(orderedValues)==0):\n",
    "        orderedValues+=[getFirstValue(X_df,suffix_feature)]\n",
    "    for columnNumber in [1, 11]:\n",
    "        while True:\n",
    "            prev_value = orderedValues[-1]\n",
    "            array_next = list(set(X_df.ix[(X_df[get_suffix(suffix_feature, columnNumber)[0]] == prev_value), [\n",
    "                get_suffix(suffix_feature, columnNumber + 1)[0]]].values.ravel()))\n",
    "            if (len(array_next) == 0):\n",
    "                break\n",
    "            elif (len(array_next) != 1):\n",
    "                raise (\"There is not one unique value\")\n",
    "            orderedValues += [array_next[0]]\n",
    "\n",
    "    index_feature = get_suffix(suffix_feature, 1)[0]\n",
    "    period_df = pd.DataFrame(\n",
    "        {index_feature: orderedValues, \"period\": range(0, len(orderedValues))}).set_index(index_feature)\n",
    "    return period_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_diffSumImports(kmt)</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-9230.9587</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11014.4854</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-3276.4590</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5655.8990</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-4455.1499</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       period\n",
       "1_diffSumImports(kmt)        \n",
       "-9230.9587                  0\n",
       " 11014.4854                 1\n",
       "-3276.4590                  2\n",
       " 5655.8990                  3\n",
       "-4455.1499                  4"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordered_values = []\n",
    "period_df = computePeriod(train_df,ordered_values)\n",
    "period_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means that by merging `train_df` with that dataFrame on the `1_diffSumImports(kmt)` columns, we would add that __`'period'` feature__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = train_df.reset_index().merge(period_df, right_index=True, how='left',left_on='1_diffSumImports(kmt)').set_index('ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "ID00001     13\n",
       "ID00002    128\n",
       "ID00003     13\n",
       "ID00004     67\n",
       "ID00005     99\n",
       "Name: period, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['period'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ordered_values` now contains all the successive values taken by the `1_diffSumImports(kmt)` column.\n",
    "With this in mind, we can call the same function on the test_df dataframe to create the __`'period'` feature__, and we can merge it similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"../data/test.csv\",delimiter=\";\",header=0,index_col=0);\n",
    "period_df_test = computePeriod(test_df,ordered_values)\n",
    "test_df = test_df.reset_index().merge(period_df_test, right_index=True, how='left',left_on='1_diffSumImports(kmt)').set_index('ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which obviously gives the same dataframe as before, but with __more periods__..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>period</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_diffSumImports(kmt)</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-9230.9587</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11014.4854</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-3276.4590</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5655.8990</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-4455.1499</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       period\n",
       "1_diffSumImports(kmt)        \n",
       "-9230.9587                  0\n",
       " 11014.4854                 1\n",
       "-3276.4590                  2\n",
       " 5655.8990                  3\n",
       "-4455.1499                  4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df ranges until period:\n",
      "period    144\n",
      "dtype: int64\n",
      "\n",
      "Whereas test_df ranges until period:\n",
      "period    170\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"train_df ranges until period:\")\n",
    "print(period_df.max())\n",
    "print(\"\\nWhereas test_df ranges until period:\")\n",
    "print(period_df_test.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking the effective continuity\n",
    "Now we can take a quick look at the continuity of our data over the periods, to __check that everything is fine__. We arbitrarily select country number 1 and look at its `diffImports` at `t = 1` and `t = 2` for each period `p in range(130,139)`.<br>\n",
    "The value at `t = 2` and `p` should be the value at `t = 1` and `p+1`.<br>\n",
    "This is what we verify with the function call below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        1_diffImports(kmt)  2_diffImports(kmt)\n",
      "period                                        \n",
      "130                 12.125             -11.625\n",
      "131                -11.625             -15.500\n",
      "132                -15.500               0.000\n",
      "133                  0.000              30.000\n",
      "        1_diffImports(kmt)  2_diffImports(kmt)\n",
      "period                                        \n",
      "134                  30.00                1.00\n",
      "135                   1.00               -4.75\n",
      "136                  -4.75                4.75\n",
      "137                   4.75              -31.00\n",
      "138                 -31.00                0.00\n"
     ]
    }
   ],
   "source": [
    "def checkPeriodContinuity(train_df, test_df):\n",
    "    def filterSort(X_df):\n",
    "        return X_df[(X_df['country'] == 1)].sort_values(by=[\"period\"]).set_index(\"period\")\n",
    "\n",
    "    def printLine(X_df_sorted):\n",
    "        start_span = 130\n",
    "        end_span = 138\n",
    "        prefix = 1\n",
    "        suffix = \"_diffImports(kmt)\"\n",
    "        print(X_df_sorted.ix[start_span:end_span, [str(prefix) + suffix, str(prefix + 1) + suffix]])\n",
    "\n",
    "    train_df_sorted = filterSort(train_df)\n",
    "    test_df_sorted = filterSort(test_df)\n",
    "    printLine(train_df_sorted)\n",
    "    printLine(test_df_sorted)\n",
    "\n",
    "checkPeriodContinuity(train_df, test_df)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes this section. We now have a `period` feature that might not be directly useful, but that might be exploited later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying countries\n",
    "#### Dummy Coding\n",
    "The `'country'` feature could be dummy coded, just like below, but this might lead to too many features, and therefore too much noise and the risk of overfitting. (And the same could be done for months)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_df = pd.get_dummies(train_df, drop_first=False, columns=['month']) # This would add 12 entries\n",
    "# train_df = pd.get_dummies(train_df, drop_first=False, columns=['country']) # This would add 76 entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be more clever to separate them somehow. Having a look at the plot below, we can spot a tendency for the points. This plot indeed separate the countries that are exporters (vertical axis), from those that are importers (horizontal axis). It is logical that a country that exports crude oil has no interest in importing it, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x121c96a58>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEACAYAAABhzAtFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119ec95f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(train_df['1_diffImports(kmt)'], train_df['1_diffExports(kmt)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below therefore creates a dataframe indexed on `'country'`, with a feature `'countryQuotient'` that separates countries into five categories, according to their ratio Imports/Exports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>countryQuotient</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>country</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3.0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4.0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5.0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        countryQuotient\n",
       "country                \n",
       "1.0                   1\n",
       "2.0                   1\n",
       "3.0                   1\n",
       "4.0                   3\n",
       "5.0                   5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeCountryQuotient(X_df):\n",
    "    countrySum = dict()\n",
    "    for columns_group in ['Imports', 'Exports']:\n",
    "        countrySum[columns_group] = (abs(X_df[[\"country\"] + get_suffix(columns_group)])).groupby(\"country\").mean().mean(\n",
    "            axis=1)\n",
    "\n",
    "    return countrySum['Imports'] / countrySum['Exports']\n",
    "\n",
    "\n",
    "def computeCategory(X_df):\n",
    "    countryQuotient = computeCountryQuotient(X_df)\n",
    "    threshold = 10\n",
    "    countryQuotient = ((countryQuotient == np.inf) + 0) + ((countryQuotient >= threshold) + 0) + (\n",
    "        (countryQuotient >= 1) + 0) \\\n",
    "                      + ((countryQuotient >= 1 / threshold) + 0) + ((countryQuotient > 0) + 0)\n",
    "    countryQuotient = countryQuotient.astype('category')\n",
    "    return pd.DataFrame.from_dict({\"countryQuotient\": countryQuotient})\n",
    "\n",
    "country_df = computeCategory(train_df)\n",
    "country_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging `country_df` with `train_df` (and `test_df`) would therefore provide the whole dataframe with the `'countryQuotient'` feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The FeatureExtractor object\n",
    " Before doing the regression, we have to choose which features to discard, to keep, or to create. This is why we need __a FeatureExtractor object__ that can do it __systematically__ for both `train_df` and `test_df`. \n",
    "The `TransformerMixin` from `sklearn` gives us the convenience to naturally call `fit_transform`.<br><br>\n",
    "As seen before, we had to merge the `period_df` and `country_df` with `train_df` and `test_df`. This should therefore be done only once in the `transform` method of the `FeatureExtractor`. (=> See the for loop)<br><br>\n",
    "At the end of the `transform` method is a selection of the features we want to keep, with `engineered_features` being an attribute of the `FeatureExtractor` object. It simply corresponds to the column names of the features we've engineered.\n",
    "\n",
    "\n",
    "+ For `period_df` this would be `'1_diffSumImports(kmt)'`\n",
    "+ For `country_df` this would be `'countryQuotient'`\n",
    "\n",
    "The `registerEngineeredFeatures` method cares about automatically adding the features name in the `engineered_features` array, and indicates how to merge the dataframe containing that specific feature. Again, __that merge operation is performed in the transform__ function.<br>\n",
    "Finally, note that the `mergeDf` function that simply does the merge we did before. It takes as a second parameter an object containing the information needed for the merge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "def createFeatureExemple(X_df,engineered_features):\n",
    "    engineered_features += [\"Exports_10_11_12\"]\n",
    "    X_df[engineered_features[-1]] = X_df[get_suffix(\"exports\", range(10,13))].sum(axis=1)\n",
    "    return X_df\n",
    "\n",
    "def mergeDf(X_df, engineered_df):\n",
    "    X_df = X_df.reset_index().merge(engineered_df[\"data\"], right_index=True, how='left',left_on=engineered_df['left_on'], left_index=engineered_df['left_index']).set_index('ID')\n",
    "    return X_df\n",
    "\n",
    "class FeatureExtractor(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.ordered_values = []\n",
    "        self.engineered_features = []\n",
    "        self.engineered_df = dict()\n",
    "\n",
    "    def fit(self, X_df, y):\n",
    "        return self\n",
    "\n",
    "    def registerEngineeredFeatures(self, feature_df, key, left_index=False, left_on='country'):\n",
    "        self.engineered_features += list(feature_df.columns)\n",
    "        self.engineered_features = list(set(self.engineered_features))\n",
    "        self.engineered_df[key] = {'data': feature_df, 'left_on': left_on, 'left_index': left_index}\n",
    "\n",
    "    def transform(self, X_df):\n",
    "        self.registerEngineeredFeatures(computePeriod(X_df, self.ordered_values, suffix_feature=\"SumImports\"), \"period\",\n",
    "                                        left_on=get_suffix(\"SumImports\", 1))\n",
    "        \n",
    "        for engineered_df in self.engineered_df.values():\n",
    "            X_df = mergeDf(X_df, engineered_df)\n",
    "\n",
    "        X_df = createFeatureExemple(X_df,self.engineered_features)\n",
    "        X_df = X_df.ix[:,  get_suffix('sumprod',[11,12])+get_suffix([\"exports\",'refinery'],[10,11,12])+ self.engineered_features]\n",
    "        return X_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is an example of the creation of a feature in the `createFeatureExemple` method.<br>\n",
    "For now, there is nothing in the `fit` method, but you can check the [Github link](https://github.com/Edouard360/crude-oil-ml/blob/master/feature_extraction/feature_extractor.py) for the full code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extractor = FeatureExtractor()\n",
    "train_df = extractor.fit_transform(train_df, train_label)\n",
    "test_df = extractor.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that in the end we end up with these predictive features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['11_diffSumProduction(kmt)', '12_diffSumProduction(kmt)',\n",
       "       '10_diffRefinery intake(kmt)', '10_diffExports(kmt)',\n",
       "       '11_diffRefinery intake(kmt)', '11_diffExports(kmt)',\n",
       "       '12_diffRefinery intake(kmt)', '12_diffExports(kmt)', 'period',\n",
       "       'Exports_10_11_12'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Regressor\n",
    "### Taking the metric into account\n",
    "For assessing the strength of a model, we need a metric. The final score is based on auc (area under curve).<br>\n",
    "Therefore, we can overwrite the `ClassifierMixin` of `sklearn` for a `ClassifierMixinAuc` that uses this metric.<br>\n",
    "We then create the associated `RandomForestClassifierAuc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "class ClassifierMixinAuc(ClassifierMixin):\n",
    "    def score(self, X, y, sample_weight=None):\n",
    "        return roc_auc_score(y,self.predict(X))\n",
    "\n",
    "class RandomForestClassifierAuc(RandomForestClassifier,ClassifierMixinAuc):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another choice of regressor\n",
    "We could also do our regression with the Gradient Boosting strategy. The `xgboost` library provides pretty powerful optimization parameters, and a convenient interface. To use it __within the `sklearn` environment__, and especially with the very useful `GridSearchCV` tool for cross validation, we need to __encapsulate__ it a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator,ClassifierMixin  \n",
    "from sklearn.exceptions import NotFittedError\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "class XGBRegressor(BaseEstimator,ClassifierMixinAuc):\n",
    "    def __init__(self,colsample_bytree=1,subsample=1,max_depth=3,min_child_weight=6,gamma=0,eta=0.3,num_round = 20): #Important to put the parameters of the base estimator here !\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        # Subsample ratio of columns when constructing each tree.\n",
    "        self.subsample = subsample\n",
    "        # Subsample ratio of the training instance.\n",
    "        # Setting it to 0.5 means that XGBoost randomly collected half of the data instances\n",
    "        # to grow trees and this will prevent overfitting.\n",
    "        self.eta = eta\n",
    "        # Step size shrinkage used in update to prevents overfitting.\n",
    "        # After each boosting step, we can directly get the weights of new features.\n",
    "        # And eta actually shrinks the feature weights to make the boosting process more conservative.\n",
    "        self.gamma = gamma\n",
    "        # Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
    "        # The larger, the more conservative the algorithm will be.\n",
    "        self.min_child_weight = min_child_weight\n",
    "        # Minimum sum of instance weight (hessian) needed in a child.\n",
    "        self.max_depth = max_depth\n",
    "        # Maximum depth of a tree\n",
    "        self.num_round = num_round\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        params = {\"colsample_bytree\": self.colsample_bytree,\n",
    "                  \"subsample\":self.subsample,\n",
    "                  \"eta\": self.eta,\n",
    "                  \"gamma\" : self.gamma,\n",
    "                  \"min_child_weight\": self.min_child_weight,\n",
    "                  \"max_depth\":self.max_depth,\n",
    "                  \"silent\":1,\n",
    "                  'objective': 'binary:logistic'}\n",
    "        self.regressor = xgb.train(params,xgb.DMatrix(X,y),num_boost_round=self.num_round)\n",
    "        self.length = X.shape[1]\n",
    "\n",
    "    @property\n",
    "    def feature_importances_(self):\n",
    "        if self.regressor is None:\n",
    "            raise NotFittedError(\"Estimator not fitted, call `fit` before\"\n",
    "                                 \" `feature_importances_`.\")\n",
    "        fscore = self.regressor.get_fscore()\n",
    "        findexes = np.sort([int(s[1:]) for s in fscore.keys()])\n",
    "        findexes_null = [i for i in range(0, self.length) if not any(findexes == i)]\n",
    "        for findindex in findexes_null:\n",
    "            fscore['f'+str(findindex)] = 0\n",
    "        importance = np.array([fscore['f' + str(i)] for i in range(0, self.length)])\n",
    "        importance = importance/importance.sum()\n",
    "        return importance\n",
    "\n",
    "    def predict(self, X):\n",
    "        pred = self.regressor.predict(xgb.DMatrix(X))\n",
    "        return (np.sign(pred - 0.5)) / 2 + 0.5\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        pred = self.regressor.predict(xgb.DMatrix(X))\n",
    "        return np.array([1 - pred, pred]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "We want a __nice print__ of our regression results, either performed with:\n",
    "\n",
    "+ A single regressor (`RandomForestClassifierAuc` or `XGBRegressor`) => `featureImportance` function.\n",
    "+ Or with `GridSearchCV` finding the best parameters for our regressor => `fitStats` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def featureImportance(fit, train_df):\n",
    "    '''\n",
    "    Given the fit returned by the fit operation of a regressor,\n",
    "    print the importance of the features and the correponding labels.\n",
    "    :param fit:\n",
    "    '''\n",
    "    sorted_importance = fit.feature_importances_.argsort()[::-1]\n",
    "    print(\"Numerical feature importance:\")\n",
    "    print(fit.feature_importances_[sorted_importance])\n",
    "    print(\"Importance ranked features:\")\n",
    "    print(train_df.columns[sorted_importance])\n",
    "\n",
    "def fitStats(fit):\n",
    "    '''\n",
    "    Given the fit returned by the fit operation of a GridSearch_CV,\n",
    "    print some stats for assessing performance.\n",
    "    :param fit:\n",
    "    '''\n",
    "    means = fit.cv_results_['mean_test_score']\n",
    "    stds = fit.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, fit.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summing up everything\n",
    "To sum it all, our main.py file simply looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.681 (+/-0.018) for {'max_depth': 10, 'n_estimators': 15}\n",
      "Numerical feature importance:\n",
      "[ 0.22586292  0.12504545  0.10049001  0.09915533  0.09649904  0.09408794\n",
      "  0.07588203  0.0742592   0.06848579  0.04023229]\n",
      "Importance ranked features:\n",
      "Index(['12_diffSumProduction(kmt)', '12_diffExports(kmt)',\n",
      "       '12_diffRefinery intake(kmt)', '11_diffSumProduction(kmt)',\n",
      "       '10_diffExports(kmt)', 'Exports_10_11_12',\n",
      "       '10_diffRefinery intake(kmt)', '11_diffRefinery intake(kmt)',\n",
      "       '11_diffExports(kmt)', 'period'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "test_df = pd.read_csv(\"../data/test.csv\", delimiter=\";\", header=0, index_col=0);\n",
    "train_df = pd.read_csv(\"../data/train_preprocessed.csv\", delimiter=\";\", header=0, index_col=0);\n",
    "train_label = pd.read_csv(\"../data/label.csv\",delimiter=\";\", header=0, index_col=0);\n",
    "\n",
    "extractor = FeatureExtractor()\n",
    "train_df = extractor.fit_transform(train_df, train_label)\n",
    "test_df = extractor.transform(test_df)\n",
    "\n",
    "param_grid = dict(max_depth=[10], n_estimators=[15])\n",
    "reg = GridSearchCV(RandomForestClassifierAuc(), param_grid=param_grid)\n",
    "  \n",
    "X_train = train_df.values\n",
    "y_train = train_label.values.ravel()\n",
    "X_test = test_df.values\n",
    "\n",
    "fit = reg.fit(X_train, y_train)\n",
    "\n",
    "fitStats(fit)\n",
    "featureImportance(fit.best_estimator_, train_df)\n",
    "\n",
    "# y_test = fit.predict_proba(X_test)[:,1]\n",
    "# pred_label = pd.DataFrame(data={'Target': y_test},index=test_df.index)\n",
    "# pred_label.to_csv(\"./output/label.csv\",sep=\";\",quotechar=\"\\\"\",quoting=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the commented lines give the output file containing our prediction of the probability for an increase in crude oil production.\n",
    "We can see the __auc score__ but also the __respective importance of the features__. That way we can choose which features to __remove__, __keep__, or __transform__. Everytime we want to edit the feature creation process, we can simply edit the FeatureExtractor object, run the program again, and see the improvements.<br>\n",
    "[Link to the repository][7]\n",
    "\n",
    "[7]: https://github.com/Edouard360/crude-oil-ml"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
